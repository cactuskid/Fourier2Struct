{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this notebook we're going to write all of the data processing steps of our pipeline\n",
    "#Inputs - Alignments with associated PDB files\n",
    "\n",
    "#Outputs - X: fourier transfor of 3d matrix of prot features. possible distmat of potts model\n",
    "#Y: distmat of 3d fold (biopython), protein class(from SCOP or CATH) ,torsion angles, ss , solvent access ( DSSP )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import wget\n",
    "import time\n",
    "import subprocess\n",
    "import shlex\n",
    "import sys\n",
    "\n",
    "from Bio.SeqUtils import seq1\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio import AlignIO\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler,  Normalizer , MinMaxScaler , RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append('./ProFET/ProFET/feat_extract/')\n",
    "import FeatureGen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA and scaler\n",
    "class NDSRobust(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._scaler = RobustScaler(copy=True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        \n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "    \n",
    "    def inverse_transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.inverse_transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "    \n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            n_dims = np.prod(self._orig_shape)\n",
    "            X = X.reshape(-1, n_dims)\n",
    "        return X\n",
    "    \n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X\n",
    "\n",
    "#ndimensional PCA for arrays\n",
    "\n",
    "class NDSPCA(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._scaler = PCA(copy = True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "    \n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        \n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        self.explained_variance_ratio_ = self._scaler.explained_variance_ratio_\n",
    "        self.components_ =self._scaler.components_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def inverse_transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.inverse_transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "    \n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            n_dims = np.prod(self._orig_shape)\n",
    "            X = X.reshape(-1, n_dims)\n",
    "        return X\n",
    "    \n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions\n",
    "##### PCA and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the components of the output space\n",
    "#stacked distmats (on the 1st axis)\n",
    "def fit_y( y , components = 300 , FFT = True ):\n",
    "    if FFT == True:\n",
    "        #got through a stack of structural distmats. these should be 0 padded to all fit in an array\n",
    "        \n",
    "        y = np.stack([ np.fft.rfft2(y[i,:,:]) for i in range(y.shape[0])] )\n",
    "        print(y.shape)\n",
    "        y =  np.hstack( [ np.real(y) , np.imag(y)]  )\n",
    "    print(y.shape)\n",
    "    ndpca = NDSPCA(n_components=components)\n",
    "    ndpca.fit(y)\n",
    "    print('explained variance')\n",
    "    print(np.sum(ndpca.explained_variance_ratio_))\n",
    "    y = ndpca.transform(y)\n",
    "    scaler0 = RobustScaler( )\n",
    "    scaler0.fit(y)\n",
    "    return scaler0, ndpca\n",
    "\n",
    "def transform_y(y, scaler0, ndpca, FFT = False):\n",
    "    if FFT == True:\n",
    "        y = np.stack([np.fft.rfft2(y[i,:,:]) for i in range(y.shape[0])])\n",
    "        print(y.shape)\n",
    "        y =  np.hstack( [ np.real(y) , np.imag(y)]  )\n",
    "    y = ndpca.transform(y)\n",
    "    print(y.shape)\n",
    "    y = scaler0.transform(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def inverse_transform_y(y, scaler0, ndpca, FFT=False):\n",
    "    y = scaler0.inverse_transform(y)\n",
    "    y = ndpca.inverse_transform(y)\n",
    "    \n",
    "    if FFT == True:\n",
    "        split = int(y.shape[1]/2)\n",
    "        y = np.stack([ np.fft.irfft2(y[i,:split,:] + 1j*y[i,split:,:]) for i in range(y.shape[0]) ] )\n",
    "        \n",
    "    return y\n",
    "\n",
    "#fit the components of the in space\n",
    "#stacked align voxels (on the 1st axis)\n",
    "def fit_x(x, components = 300, FFT = True):\n",
    "    if FFT == True:\n",
    "        #got through a stack of align voxels. these should be 0 padded to all fit in an array\n",
    "        \n",
    "        x = np.stack([ np.fft.rfftn(x[i,:,:,:]) for i in range(x.shape[0])] )\n",
    "        print(x.shape)\n",
    "        x =  np.hstack( [ np.real(x) , np.imag(x)]  )\n",
    "    print(x.shape)\n",
    "    ndpca = NDSPCA(n_components=components)\n",
    "    ndpca.fit(x)\n",
    "    print('explained variance')\n",
    "    print(np.sum(ndpca.explained_variance_ratio_))\n",
    "    x = ndpca.transform(x)\n",
    "    scaler0 = RobustScaler( )\n",
    "    scaler0.fit(x)\n",
    "    return scaler0, ndpca\n",
    "\n",
    "def transform_x(x, scaler0, ndpca, FFT = False):\n",
    "    if FFT == True:\n",
    "        x = np.stack([ np.fft.rfftn(x[i,:,:,:]) for i in range(x.shape[0])] )\n",
    "        print(x.shape)\n",
    "        x =  np.hstack( [ np.real(x) , np.imag(x)]  )\n",
    "    x = ndpca.transform(x)\n",
    "    print(x.shape)\n",
    "    x = scaler0.transform(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "#todo -- check the split is happening in the right dimension\n",
    "def inverse_transform_x(x, scaler0, ndpca, FFT=False):\n",
    "    x = scaler0.inverse_transform(x)\n",
    "    x = ndpca.inverse_transform(x)\n",
    "    \n",
    "    if FFT == True:\n",
    "        split = int(x.shape[1]/2)\n",
    "        x = np.stack([ np.fft.irfftn(x[i,:split,:,:] + 1j*x[i,split:,:,:]) for i in range(x.shape[0]) ] )\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get align files\n",
    "def runclustalo( infile , runIdentifier, path = 'clustalo' , outdir='./', args = '' , verbose = False):\n",
    "    if verbose == True:\n",
    "        print( infile , runIdentifier , path , outdir )\n",
    "    #i usually use filenames that reflect what the pipeline has done until that step\n",
    "    outfile= outdir+runIdentifier+infile+\".aln.fasta\"\n",
    "    \n",
    "    #here we write the command as a string using all the args\n",
    "    args = path + ' -i '+  infile  +' -o '+ outfile + ' ' +args\n",
    "    args = shlex.split(args)\n",
    "    if verbose == True:\n",
    "        print(args)\n",
    "    p = subprocess.Popen(args )\n",
    "    #return the opened process and the file it's creating\n",
    "    \n",
    "    #we can also use the communicate function later to grad stdout if we need to\n",
    "    return p , outfile\n",
    "\n",
    "#TODO - add sequence to align\n",
    "\n",
    "def alnFileToArray(filename, returnMsa = False):\n",
    "    alnfile = filename\n",
    "    msa = AlignIO.read(alnfile , format = 'fasta')\n",
    "    align_array = np.array([ list(rec.upper())  for rec in msa], np.character)\n",
    "    \n",
    "    if returnMsa:\n",
    "        return align_array, msa\n",
    "        \n",
    "    return align_array\n",
    "\n",
    "#generate align list\n",
    "def generateAlignList(directory = 'alns', returnMsa = False):\n",
    "    aligns = list()\n",
    "    msas = list()\n",
    "    \n",
    "    #read through align files to get align arrays list\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.fasta'):\n",
    "            aligns.append(alnFileToArray(directory+'/'+file, returnMsa)[0])\n",
    "            if returnMsa:\n",
    "                msas.append(alnFileToArray(directory+'/'+file, returnMsa)[1])\n",
    "    \n",
    "    if returnMsa:\n",
    "        return aligns, msas\n",
    "    \n",
    "    return aligns\n",
    "\n",
    "#find biggest align shape (for padding) - aligns is a list of arrays\n",
    "def biggestAlignShape(aligns):\n",
    "    longestProts = 0\n",
    "    mostProts = 0\n",
    "\n",
    "    for aln in aligns:\n",
    "        if aln.shape[0] > mostProts:\n",
    "            mostProts = aln.shape[0]\n",
    "        if aln.shape[1] > longestProts:\n",
    "            longestProts = aln.shape[1]\n",
    "    \n",
    "    return mostProts, longestProts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rundssp( infile , runIdentifier, path = 'dssp' , outdir='./', args = '' , verbose = False):\n",
    "    if verbose == True:\n",
    "        print( infile , runIdentifier , path , outdir )\n",
    "    #i usually use filenames that reflect what the pipeline has done until that step\n",
    "    outfile= outdir+runIdentifier+infile+\".dssp\"\n",
    "    \n",
    "    #here we write the command as a string using all the args\n",
    "    args = path + ' -i '+  infile  +' -o '+ outfile + ' ' +args\n",
    "    args = shlex.split(args)\n",
    "    if verbose == True:\n",
    "        print(args)\n",
    "    p = subprocess.Popen(args)\n",
    "    #return the opened process and the file it's creating\n",
    "    \n",
    "    #we can also use the communicate function later to grad stdout if we need to\n",
    "    return p , outfile\n",
    "\n",
    "def dssp2pandas(dsspstr):\n",
    "    #read the dssp file format into a pandas dataframe\n",
    "    start = False\n",
    "    lines = {}\n",
    "    count = 0\n",
    "    for l in dsspstr.split('\\n'):\n",
    "        if '#' in l:\n",
    "            start = True\n",
    "        if start == True:\n",
    "            if count > 0:\n",
    "                lines[count] = dict(zip(header,l.split()))\n",
    "            else:\n",
    "                header = l.split()\n",
    "            count +=1\n",
    "    df = pd.DataFrame.from_dict( lines , orient = 'index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDB parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structs is a dictionary of all the structures (which are then subdivided into chains)\n",
    "def parsePDB(structs):\n",
    "    parser = PDBParser()\n",
    "    converter = {'ALA': 'A', 'ASX': 'B', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F', 'GLY': 'G',\n",
    "                 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P',\n",
    "                 'GLN': 'Q', 'ARG': 'R', 'SER': 'S', 'THR': 'T', 'SEC': 'U', 'VAL': 'V', 'TRP': 'W',\n",
    "                 'XAA': 'X', 'TYR': 'Y', 'GLX': 'Z'}\n",
    "    structseqs={}\n",
    "    with open( 'structs.fast' , 'w') as fastout:\n",
    "        for s in structs:\n",
    "            Structure = PDBParser().get_structure(s, structs[s])\n",
    "            for model in Structure:\n",
    "                for chain in model:\n",
    "                    res = chain.get_residues()\n",
    "                    seq =  ''.join([ converter[r.get_resname()] for r in res if r.get_resname() in converter ] )\n",
    "                    fastout.write('>' + s + '|'+ chain.id +'\\\\n')\n",
    "                    fastout.write(str( seq ) +'\\\\n'  )\n",
    "                    structseqs[ s + '|'+ chain.id ] = seq\n",
    "    \n",
    "    return structseqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ProtFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProtFeatDict(sequence):\n",
    "    features = FeatureGen.Get_Protein_Feat(sequence)\n",
    "    return features\n",
    "\n",
    "#generate complete set of dictionary keys generated by protFET\n",
    "def protFeatKeys(align_array, msa):\n",
    "    dictKeys = set()\n",
    "\n",
    "    for i in range(align_array.shape[0]):\n",
    "        sequence = str(msa[i].seq)\n",
    "        sequence = sequence.replace('.', '')\n",
    "        sequence = sequence.replace('-','')\n",
    "        dictKeys = dictKeys.union(set(generateProtFeatDict(sequence).keys()) - dictKeys)\n",
    "    \n",
    "    return dictKeys\n",
    "    \n",
    "#generate ProtFET array for given align (maxKeys: all keys of the feature dictionary, over the entire set)\n",
    "def alignToProtFeat(align_array, msa, dictKeys):\n",
    "    #generate 2d array of ProtFET features for each sequence in align\n",
    "    align_features = np.zeros((align_array.shape[0], len(dictKeys)), dtype=float)\n",
    "    missingFeatures = set()\n",
    "\n",
    "    for i in range(align_array.shape[0]):\n",
    "        sequence = str(msa[i].seq)\n",
    "        sequence = sequence.replace('.', '')\n",
    "        sequence = sequence.replace('-','')\n",
    "        featuresDict = generateProtFeatDict(sequence)\n",
    "        missingFeatures = dictKeys - set(featuresDict.keys())\n",
    "        for newKey in missingFeatures:\n",
    "            featuresDict[newKey] = float(0)\n",
    "        features = np.array(list(featuresDict.values()))\n",
    "        align_features[i,:] = features\n",
    "        \n",
    "    return align_features\n",
    "\n",
    "#generate array of ProtFeat features for all aligns\n",
    "def protFeatArrays(aligns, msas):\n",
    "    maxKeys = set()\n",
    "    mostProts = biggestAlignShape(aligns)[0]\n",
    "    \n",
    "    #build set of all keys used in the set\n",
    "    for i in range(len(aligns)):\n",
    "        maxKeys = maxKeys.union(protFeatKeys(aligns[i], msas[i]) - maxKeys)\n",
    "           \n",
    "    setFeatures = np.zeros((len(aligns), mostProts, len(maxKeys)))\n",
    "    for i in range(len(aligns)):\n",
    "        np.append(setFeatures, alignToProtFeat(aligns[i], msas[i], maxKeys))\n",
    "    \n",
    "    return setFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAlignVoxel(align_array):\n",
    "    align_prop_array = np.zeros((align_array.shape[0], align_array.shape[1], len(numerical)), dtype=float)\n",
    "\n",
    "    for i in range(align_array.shape[0]):\n",
    "        align_prop_array[i,:,:] = [[properties[prop][bstring] for prop in numerical] for bstring in align_array[i]]\n",
    "    \n",
    "    return align_prop_array\n",
    "\n",
    "#generate 4D array of stacked 3D voxels for FFT (and PCA)\n",
    "def generateVoxelArray(aligns, propAmount = 12):\n",
    "    #find biggest align_array (the depth of the voxel is fixed by the number of properties)\n",
    "    mostProts, longestProts = biggestAlignShape(aligns)\n",
    "\n",
    "    #pad all aligns (with 'b'.) to be the same size\n",
    "    for i in range(len(aligns)):\n",
    "        padded = np.full((mostProts, longestProts), b'.')\n",
    "        padded[:aligns[i].shape[0],:aligns[i].shape[1]] = aligns[i]\n",
    "        aligns[i] = padded\n",
    "\n",
    "    #generate voxel array\n",
    "    voxels = np.zeros((len(aligns), mostProts, longestProts, propAmount))\n",
    "    for i in range(len(aligns)):\n",
    "        voxels[i, :, :, :] = generateAlignVoxel(aligns[i])\n",
    "    \n",
    "    return voxels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#builds a dictionary of distmats in the set - structs is a dictionary of all the structures (which are then subdivided into chains)\n",
    "def PDBToDistmat(structs, show = False):\n",
    "    distances = {}\n",
    "    for s in structs:\n",
    "        Structure = PDBParser().get_structure(s, structs[s])\n",
    "        distances[s] = {}\n",
    "        for model in Structure:\n",
    "            for chain in model:\n",
    "                res = [r for r in chain.get_residues()]\n",
    "                distmat = [ [res2['CA'] - res1['CA'] if 'CA' in res1 and 'CA' in res2 and i > j else 0 for i,res1 in enumerate(res)] for j,res2 in enumerate(res)]\n",
    "                distmat = np.array(distmat)\n",
    "                distmat+= distmat.T\n",
    "                distances[s][chain] = distmat\n",
    "\n",
    "    if show:\n",
    "        for s in distances:\n",
    "            print(s)\n",
    "            for c in distances[s]:\n",
    "                sns.heatmap(distances[s][c])\n",
    "                plt.show()\n",
    "    \n",
    "    return distances\n",
    "\n",
    "#builds 3D array of all distmats in the set\n",
    "def distmatDictToArray(distances):\n",
    "    #make list of proteins, containing list of distance arrays for each chain\n",
    "    protChainsList = list()\n",
    "    chainDistArrayList = list()\n",
    "\n",
    "    for protein in distances:\n",
    "        for chain in distances[protein]:\n",
    "            distArray = np.array(distances[protein][chain])\n",
    "            if np.sum(distArray) != 0:   #if we leave empty chains, the pca's variance calculations don't work (division by 0)\n",
    "                chainDistArrayList.append(distArray)\n",
    "        protChainsList.append(chainDistArrayList)\n",
    "        chainDistArrayList = list()\n",
    "\n",
    "    #preserve original shape before flattening (not needed for now, but might be useful later)\n",
    "    chainAmounts = np.array((len(protChainsList),1), dtype=int)\n",
    "    for i in range(len(protChainsList)):\n",
    "        chainAmounts[i] = len(protChainsList[i])\n",
    "\n",
    "    #flatten 2D list into 1D list\n",
    "    arrayList = list()\n",
    "    [[arrayList.append(protChainsList[i][j]) for j in range(chainAmounts[i])] for i in range(len(protChainsList))]\n",
    "\n",
    "    #find size of the largest distmat\n",
    "    maxX, maxY = biggestAlignShape(arrayList)\n",
    "\n",
    "    #pad the arrays so they're all the same size\n",
    "    for i in range(len(arrayList)):\n",
    "        padded = np.zeros((maxX, maxY))\n",
    "        padded[:arrayList[i].shape[0], :arrayList[i].shape[1]] = arrayList[i]\n",
    "        arrayList[i] = padded\n",
    "\n",
    "    #make 3D array of all distmats in the set\n",
    "    distmats = np.zeros((len(arrayList), maxX, maxY))\n",
    "\n",
    "    for mat in arrayList:\n",
    "        distmats[i,:,:] = arrayList[i]\n",
    "    \n",
    "    return distmats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [ 'alns' , 'templates' , 'TensorflowModels' ]\n",
    "clear = False\n",
    "\n",
    "for path in folders:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    if clear == True:\n",
    "        files = glob.glob(path+'*.pdb')\n",
    "        for f in files:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AA property dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "propfile = './physicalpropTable.csv'\n",
    "propdf = pd.read_csv(propfile)\n",
    "\n",
    "numerical = [ 'pKa side chain', 'pka2', 'pka3',\n",
    "              'PI', 'Solubility Molal', 'MW', 'charge', 'ww hydrophob scale',\n",
    "              'hydr or amine', 'aliphatic', 'aromatic', 'hydrophobicity at ph7']\n",
    "properties = { prop: dict(zip(propdf['letter Code' ] , propdf[prop] ) ) for prop in numerical }\n",
    "properties = { prop:{c.encode(): properties[prop][c] for c in properties[prop]} for prop in properties}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download of the PDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./templates/1OK8.pdb', './templates/1OAN.pdb']\n"
     ]
    }
   ],
   "source": [
    "#TODO - automate getting the model names\n",
    "models = '1ok8 1oan'.split()\n",
    "\n",
    "dl_url = 'http://files.rcsb.org/download/'\n",
    "dl_url_err = 'http://files.rcsb.org/download/'\n",
    "structs = {}\n",
    "already = glob.glob( './templates/*.pdb' )\n",
    "print(already)\n",
    "\n",
    "#pull complexes\n",
    "for m in models:\n",
    "    structfile = './templates/'+m.upper().strip()+'.pdb'\n",
    "    if structfile not in already:\n",
    "        print(m)\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            wget.download(url = dl_url + m.strip() +'.pdb' , out =structfile)\n",
    "            structs[m] = structfile\n",
    "        except:\n",
    "            try:\n",
    "                wget.download(url = dl_url + m.strip() +'.pdb' , out =structfile)\n",
    "                structs[m] = structfile\n",
    "            except:\n",
    "                print('err', m )\n",
    "    else:\n",
    "        structs[m.strip()] = structfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#runclustalo( infile , runIdentifier, path = 'clustalo' , outdir='./', args = '' , verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6846.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 6861.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6875.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 6895.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Converting `np.character` to a dtype is deprecated. The current result is `np.dtype(np.str_)` which is not strictly correct. Note that `np.character` is generally deprecated and 'S1' should be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./templates/1OK8.pdb\n",
      "./templates/1OK8.pdb test dssp ./templates/\n",
      "['dssp', '-i', './templates/1OK8.pdb', '-o', './templates/test./templates/1OK8.pdb.dssp']\n",
      "./templates/1OAN.pdb\n",
      "./templates/1OAN.pdb test dssp ./templates/\n",
      "['dssp', '-i', './templates/1OAN.pdb', '-o', './templates/test./templates/1OAN.pdb.dssp']\n",
      "['./templates/1OAN.pdb.dssp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6846.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 6861.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6875.\n",
      "  PDBConstructionWarning,\n",
      "/home/osboxes/miniconda3/envs/structML/lib/python3.7/site-packages/Bio/PDB/StructureBuilder.py:92: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 6895.\n",
      "  PDBConstructionWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25, 144, 12)\n"
     ]
    }
   ],
   "source": [
    "structseqs = parsePDB(structs)\n",
    "aligns, msas = generateAlignList(directory = 'alns', returnMsa = True)\n",
    "\n",
    "#where do we use the dssp dataframes?\n",
    "#TODO - make into functions\n",
    "for s in structs:\n",
    "    print(structs[s])\n",
    "    p, outdssp = rundssp( structs[s] , 'test' , outdir = './templates/' , verbose = True)\n",
    "    p.wait()\n",
    "    \n",
    "dssps= glob.glob( './templates/*.dssp')\n",
    "print(dssps)\n",
    "for dssp in dssps:\n",
    "    with open( dssp , 'r') as dsspin:\n",
    "        df = dssp2pandas( dsspin.read() )\n",
    "\n",
    "distances = PDBToDistmat(structs, show = False)\n",
    "features = protFeatArrays(aligns, msas)\n",
    "distmats = distmatDictToArray(distances)\n",
    "voxels = generateVoxelArray(aligns, propAmount = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate sequences, adding missing distmat sequences to alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data (remove sequences with x% identity, filter according to groups...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final touches: PCA and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOXELS: \n",
      "(2, 25, 144, 7)\n",
      "(2, 50, 144, 7)\n",
      "explained variance\n",
      "1.0\n",
      "(2, 25, 144, 7)\n",
      "(2, 2)\n",
      "DISTMATS: \n",
      "(3, 587, 294)\n",
      "(3, 1174, 294)\n",
      "explained variance\n",
      "1.0\n",
      "(3, 587, 294)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"VOXELS: \")\n",
    "scalerX, ndpcaX = fit_x(voxels, components = 2, FFT = True)\n",
    "transformed_voxels = transform_x(voxels, scalerX, ndpcaX, FFT = True)\n",
    "\n",
    "print(\"DISTMATS: \")\n",
    "scalerY, ndpcaY = fit_y(distmats, components = 3, FFT = True)\n",
    "transformed_distmats = transform_y(distmats, scalerY, ndpcaY, FFT = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
