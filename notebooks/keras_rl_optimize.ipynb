{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddae15e-56eb-4de2-8d7a-bc21b367c621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (0.4.2)\n",
      "Requirement already satisfied: gym in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (0.18.0)\n",
      "Requirement already satisfied: PeptideBuilder in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (1.1.0)\n",
      "Collecting keras-transformer\n",
      "  Downloading keras-transformer-0.38.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from gym) (1.20.1)\n",
      "Requirement already satisfied: scipy in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from gym) (1.6.2)\n",
      "Requirement already satisfied: future in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: keras>=2.0.7 in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from keras-rl) (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from keras>=2.0.7->keras-rl) (5.4.1)\n",
      "Requirement already satisfied: h5py in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from keras>=2.0.7->keras-rl) (2.10.0)\n",
      "Collecting keras-pos-embd>=0.11.0\n",
      "  Downloading keras-pos-embd-0.11.0.tar.gz (5.9 kB)\n",
      "Collecting keras-multi-head>=0.27.0\n",
      "  Downloading keras-multi-head-0.27.0.tar.gz (14 kB)\n",
      "Collecting keras-layer-normalization>=0.14.0\n",
      "  Downloading keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\n",
      "Collecting keras-position-wise-feed-forward>=0.6.0\n",
      "  Downloading keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\n",
      "Collecting keras-embed-sim>=0.8.0\n",
      "  Downloading keras-embed-sim-0.8.0.tar.gz (4.1 kB)\n",
      "Collecting keras-self-attention==0.46.0\n",
      "  Downloading keras-self-attention-0.46.0.tar.gz (10 kB)\n",
      "Requirement already satisfied: Biopython in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from PeptideBuilder) (1.78)\n",
      "Requirement already satisfied: six in /scratch/dmoi/miniconda/envs/ML/lib/python3.9/site-packages (from h5py->keras>=2.0.7->keras-rl) (1.15.0)\n",
      "Building wheels for collected packages: keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-self-attention, keras-pos-embd, keras-position-wise-feed-forward\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-py3-none-any.whl size=12941 sha256=9929762c02008c9fa298b58e93a83cf20dc2f873a9d75703ff5ef83368090724\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/12/e0/8b/0687ec6c78fe9e6ae302d0247fe66672e46ce67ca0d330f120\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-py3-none-any.whl size=4557 sha256=15d1eba3afc250ed8f0f1c0d98642039e6a6c510d02a7b264439bb2abbe4c676\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/06/31/b5/22ec049def2c6c231612294735129d113a221d4aca64d28443\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-py3-none-any.whl size=5267 sha256=f97437be2e8e0b1338ce2b3fe46043edcf5926a39307d636dfcc7dd47c60cb84\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/33/2f/ec/4bb11b7132f5f9edb662cc4ffc2ccbbd560c67f93c80c9d71a\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-py3-none-any.whl size=15615 sha256=29f20afb574247aa00b2044c74fccceeb8df2921a834136eb66a6bd1bcb0d734\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/a1/f2/af/d2f2084e37ef8d7d85a456807bf31caac4078e1111d71838ec\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-py3-none-any.whl size=17277 sha256=554fb279267a6cdabe9da3302e01fc28709021a8d8dc3a998f20eea61b79f1e9\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/7e/f3/6a/26d33e622a8e1fc9ff5f18a541c3292ce6d197b8fcd58942ec\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-py3-none-any.whl size=7552 sha256=e69548bc5de288caeaf731b6815b62ffadd2e761db01873ab6de29e55d1d39fd\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/8f/95/2a/876a06e4f88aac30e4e71f450687e0693bff6e8e0ae45a78cb\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-py3-none-any.whl size=5623 sha256=026bc99df218838bea03579a8713346df31b1c99e58c485a185afd3e37daceb5\n",
      "  Stored in directory: /users/dmoi/.cache/pip/wheels/f7/07/78/dda198c2eec189df7e9f3bf9c6364acbdf6e1d9c66ecbb9bb7\n",
      "Successfully built keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-self-attention keras-pos-embd keras-position-wise-feed-forward\n",
      "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer\n",
      "Successfully installed keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-rl gym PeptideBuilder keras-transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12602794-a1e5-4026-9478-c9972cff8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create a peptide using a distmat and sequence to start\n",
    "\n",
    "\"\"\"\n",
    "Simple example script demonstrating how to use the PeptideBuilder library.\n",
    "The script generates a peptide consisting of six arginines in alpha-helix\n",
    "conformation, and it stores the peptide under the name \"example.pdb\".\n",
    "\"\"\"\n",
    "\n",
    "from PeptideBuilder import Geometry\n",
    "import PeptideBuilder\n",
    "import Bio.PDB\n",
    "\n",
    "\n",
    "# create a peptide consisting of 6 glycines\n",
    "geo = Geometry.geometry(\"G\")\n",
    "geo.phi = -60\n",
    "geo.psi_im1 = -40\n",
    "structure = PeptideBuilder.initialize_res(geo)\n",
    "for i in range(5):\n",
    "    PeptideBuilder.add_residue(structure, geo)\n",
    "# add terminal oxygen (OXT) to the final glycine\n",
    "PeptideBuilder.add_terminal_OXT(structure)\n",
    "\n",
    "\n",
    "out = Bio.PDB.PDBIO()\n",
    "out.set_structure(structure)\n",
    "out.save(\"example.pdb\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d2749-d4d7-4e79-ad7f-81ac10fd868d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054b6c0-27e8-414a-b186-e2b6a02c615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_v(struct):\n",
    "    #use pyrosetta and other potentials\n",
    "    return v\n",
    "\n",
    "def calculate_localv(struct,res):\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078a07e-ec13-41ae-b662-22948778ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project pairwise euclidean distance down to a 3d point cloud\n",
    "\n",
    "def init_trajectory():\n",
    "    \n",
    "    #convert the distmat to a series of 3d points\n",
    "    \n",
    "    mds = sklearn.manifold.MDS(n_components=3, *, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=-1, random_state=None, dissimilarity='precomputed')\n",
    "    N_coords = mds.fit_transform(distmatN)\n",
    "    C1_coords = mds.fit_transform(distmatC)\n",
    "    CA_coords = mds.fit_transform(distmatCA)\n",
    "\n",
    "    #align the centers of gravity\n",
    "    \n",
    "    #rotate to find the min rmsd\n",
    "     \n",
    "    #derive bond angles \n",
    "    \n",
    "    #create rough peptide with bond angles\n",
    "    \n",
    "    #add the first n timesteps with the first frame\n",
    "    pass\n",
    "    \n",
    "def timestep():\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    #pop the oldest trajectory entry from the vec passed to the qnet\n",
    "    #recalculate the new struct using the angle increments\n",
    "    #calculate dssp and rosetta scores for each res\n",
    "    \n",
    "    \n",
    "    #return trajectory with new timestep to qnet\n",
    "    #append timestep to total trajectory\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def return_err( pred_struct, true_struct ):\n",
    "    \n",
    "    #calculate error for each angle\n",
    "    #calculate global error metric like tm score\n",
    "    \n",
    "    pass\n",
    "    \n",
    "    \n",
    "    \n",
    "#dqtn where each residue gets a total score, local score and angles, local dssp, for the last n timesteps\n",
    "#for the first iteration just pad with 3x the first input and flush them out as it progresses\n",
    "\n",
    "\n",
    "def show_traj():\n",
    "    #show n timesteps using nglviewer\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec779d-b0e4-4de6-891c-71bfbe36d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=1):\n",
    "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # trend\n",
    "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        # periodic\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp) # or K.cos(.)\n",
    "\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        return ret\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd154b52-4719-4cb8-94f1-efe849ecc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate distmat\n",
    "\n",
    "dist = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6f08d-f2e0-45a3-bee5-f154cb5a2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dssp to pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "def rundssp( infile , runIdentifier, path = 'dssp' , outdir='./', args = '' , verbose = False):\n",
    "    if verbose == True:\n",
    "        print( infile , runIdentifier , path , outdir )\n",
    "    #i usually use filenames that reflect what the pipeline has done until that step\n",
    "    outfile= outdir+runIdentifier+infile+\".dssp\"\n",
    "    \n",
    "    #here we write the command as a string using all the args\n",
    "    args = path + ' -i '+  infile  +' -o '+ outfile + ' ' +args\n",
    "    args = shlex.split(args)\n",
    "    if verbose == True:\n",
    "        print(args)\n",
    "    p = subprocess.Popen(args)\n",
    "    #return the opened process and the file it's creating\n",
    "    \n",
    "    #we can also use the communicate function later to grad stdout if we need to\n",
    "    return p , outfile\n",
    "\n",
    "\n",
    "def dssp2pandas(dsspstr):\n",
    "    #read the dssp file format into a pandas dataframe\n",
    "    start = False\n",
    "    lines = {}\n",
    "    count = 0\n",
    "    for l in dsspstr.split('\\n'):\n",
    "        if '#' in l:\n",
    "            start = True\n",
    "        if start == True:\n",
    "            if count > 0:\n",
    "                lines[count] = dict(zip(header,l.split()))\n",
    "            else:\n",
    "                header = l.split()\n",
    "            count +=1\n",
    "    df = pd.DataFrame.from_dict( lines , orient = 'index')\n",
    "    return df\n",
    "\n",
    "\n",
    "dssps= glob.glob( './templates/*.dssp')\n",
    "print(dssps)\n",
    "for dssp in dssps:\n",
    "    with open( dssp , 'r') as dsspin:\n",
    "        df = dssp2pandas( dsspin.read() )\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1eebb-2c1b-46f8-97d2-a69c1726f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#not working at the moment due to permissions for some reason...\n",
    "for s in structs:\n",
    "    print(structs[s])\n",
    "    p, outdssp = rundssp( structs[s] , 'test' , outdir = './templates/' , verbose = True)\n",
    "    p.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a586825-1e8c-45a7-9a7f-601c358ba190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58e9a2-2a80-452f-b8be-350a458a7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a gym folding env\n",
    "#state is the distmat and the score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce33ea-08cf-4f4c-a758-f27ed4385773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#use alpha go strategy - decision tree pruning w score\n",
    "#feed it the current distmat, start, local score and global score\n",
    "#train on folding agent and paralellize over decision tree\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
